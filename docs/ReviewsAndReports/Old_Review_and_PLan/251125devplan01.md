# 251125 Development Plan — Agent Gateway Drop-In Remediation

Goal: Restore the promised UX where any OpenAI Agents SDK module dropped under `src/agents/**` is discoverable, secure, tool-capable, and observable via `/v1/chat/completions`, with CI and metrics you can trust.

Scope: Core runtime and ops surfaces (discovery/registry, executor + tool loop, SDK adapter, streaming, metrics, watch mode, security pattern matching). Examples directory is out of scope per request.

Progress Tracker (update as work completes)
- [x] Step 1 — Fix broken tests/fixtures
- [x] Step 2 — Registry cleanup & discovery telemetry
- [x] Step 3 — Tool definitions in upstream payloads & tool-call routing
- [x] Step 4 — Real streaming passthrough (declarative + SDK)
- [x] Step 5 — SDK adapter upgrades (context, async, gateway tools)
- [x] Step 6 — Metrics/admin expansion for tools & discovery
- [x] Step 7 — Watch-mode coverage & security pattern alignment

---

## Step 1 — Clean broken tests/fixtures so CI is trustworthy
**Why:** Patch artifacts currently break collection, hiding regressions.
**Tasks:**
1) Remove stray `*** End Patch` markers and malformed content in `tests/test_smoke_gateway.py` and `tests/fixtures/async_sdk_agent.py`; ensure files are valid Python.
2) Re-run `make test` and `make test-acceptance` locally to confirm collection succeeds.
3) If additional fixtures rely on corrupted files, regenerate them or add minimal working stubs.
**Exit criteria:** Full test collection passes; smoke and drop-in acceptance tests run without syntax errors.

## Step 2 — Fix registry cleanup and discovery telemetry
**Why:** Agents silently drop when defaults are missing; bad destructor can raise at GC; diagnostics aren’t surfaced.
**Tasks:**
1) Remove or rewrite `AgentRegistry.__del__` (undefined variables). Move any intended metric/error recording into `_record_diagnostic` or watch loop.
2) In `_spec_from_export`, if upstream/model defaults are missing, emit to `error_recorder` and `metrics.record_dropin_failure(kind="discovery_validation")` and include in `/admin/agents/errors`.
3) Add fallbacks from `Settings` (`GATEWAY_DEFAULT_UPSTREAM`, `GATEWAY_DEFAULT_MODEL`, namespace default) before dropping an agent.
4) Ensure discovery diagnostics are propagated to `/admin/agents/errors` and persisted in-memory for listing; include source file, module, reason.
5) Add/adjust unit tests for: missing defaults surfacing diagnostics; security-blocked modules recorded; destructor removal.
**Exit criteria:** Agents without explicit defaults surface actionable errors; no silent drops; no `__del__` NameError; tests cover telemetry paths.

## Step 3 — Wire tool definitions into upstream payloads and route returned tool calls through `tool_manager`
**Why:** Declarative agents currently send no tool schemas; upstreams can’t emit tool calls and gateway-managed tools never run.
**Tasks:**
1) When building payload in `AgentExecutor._build_payload`, inject tool definitions for declarative agents from `tool_manager.list_tools()` matching `agent.tools`. Honor `tool_choice` defaults (e.g., auto vs required) if present in agent metadata or request.
2) Normalize tool schema to OpenAI-compatible format, including function names, descriptions, and JSON schema derived from `ToolsFile` entries.
3) On receiving tool-call deltas from upstream, ensure responses flow into `_execute_tool_calls` and back into the message stack with correct `tool_call_id` and serialized outputs.
4) Add validation to reject unknown tool names before upstream invocation and surface clear 4xx/5xx errors.
5) Expand smoke/acceptance tests to assert tool defs are present in upstream payloads and that tool_manager is invoked end-to-end.
**Exit criteria:** Upstream requests include tool schemas; returned tool_calls execute via `tool_manager` with security/metrics; tests cover the flow.

## Step 4 — Add real streaming passthrough (declarative + SDK)
**Why:** Current streaming is synthetic (post-hoc chunking); tool-call deltas and upstream streams aren’t forwarded.
**Tasks:**
1) For declarative agents, use upstream streaming (`client.chat.completions.create(stream=True, ...)`) and pipe chunks directly to the HTTP stream, translating tool-call deltas into SSE events.
2) Ensure backpressure-safe iteration and proper termination with `[DONE]` while keeping request IDs in context.
3) For SDK agents, support streaming results from OpenAI Agents SDK when available; otherwise, surface a clean “non-streaming agent” error instead of faking chunks.
4) Update `api/services/streaming.py` to handle true streaming payloads and fall back only when upstream streaming is unavailable.
5) Add streaming tests (unit/integration) that assert deltas pass through and tool calls can be initiated mid-stream.
**Exit criteria:** Clients receive true streaming deltas (including tool_call chunks); synthetic chunking is only a fallback; tests verify streaming paths.

## Step 5 — Upgrade the SDK adapter for OpenAI Agents
**Why:** Context is collapsed; `asyncio.run` anti-pattern; gateway tool telemetry bypassed for native tools; no streaming support.
**Tasks:**
1) Preserve the full message stack when invoking SDK agents (convert to `TResponseInputItem` instead of single prompt collapse).
2) Replace `asyncio.run` in threads with proper async handling or dedicated event loop helpers to avoid deadlocks.
3) Add streaming support for SDK agents that emit deltas; propagate them to HTTP streaming with request IDs.
4) Enforce or wrap native `function_tool` calls so gateway-managed tools record metrics/ACLs; consider shim to auto-wrap SDK tools or document/guardrail.
5) Expand tests to cover: multi-turn context retention, async agents (including `tests/fixtures/async_sdk_agent.py`), streaming, and gateway-tool enforcement.
**Exit criteria:** SDK agents behave like in Agent Builder (contextful, streaming-capable), and tool usage is observable/controlled via gateway tooling; tests pass.

## Step 6 — Expand `/admin/metrics` (and docs) to surface tool & discovery stats
**Why:** Operators can’t see tool invocation/failure counts or drop-in failures via the admin API.
**Tasks:**
1) Extend `MetricsResponse` and `/admin/metrics` to return the full `GatewayMetrics.snapshot()` payload (tool breakdown, failures, drop-in failure counts, min/avg/max latencies).
2) Align Prometheus exporters to ensure labels/fields match the admin surface; document metric names and meanings.
3) Update docs (README + relevant guides) to describe new metrics fields and example queries.
4) Add tests that assert `/admin/metrics` includes tool/discovery data and Prometheus generation remains valid.
**Exit criteria:** Admin consumers see tool/discovery metrics; Prometheus mirrors the same semantics; tests cover the expanded shape.

## Step 7 — Improve watch-mode coverage & security pattern matching
**Why:** Watch mode misses helper/requirements changes; security patterns are more limited than docs imply.
**Tasks:**
1) Expand watch to react to changes in supporting files (e.g., helper modules, `requirements.txt`) or clearly document the limitation in guides.
2) Consider incremental refresh on directory mtimes (not just `agent.py`) when watch is enabled; ensure cache invalidation works.
3) Enhance `AuthContext._match_pattern`/security manager to support documented glob-style patterns (or update docs to reflect actual matching). Add tests for new patterns.
4) Update `docs/guides/DropInAgentGuide.md` and `SDKOnboarding.md` to clarify watch-mode behavior and security pattern rules.
**Exit criteria:** Watch mode behavior is predictable (either expanded or well-documented); security pattern matching aligns with documentation; tests cover new patterns.

---

How to use this plan: work top-to-bottom, updating the progress tracker as steps complete. Keep tests and docs current at each step to preserve the drop-in UX.
